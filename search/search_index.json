{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Apache Nifi","text":""},{"location":"#construindo-o-fluxo","title":"Construindo o Fluxo","text":"<p>Os dados utilizados no exemplo v\u00eam de duas fontes:</p> <ul> <li>Arquivo de dados sobre os CIDs [1], dispon\u00edvel na pasta data, da raiz do reposit\u00f3rio.</li> <li>Dados sobre a mortalidade, em 2020 [2]</li> </ul>"},{"location":"#fluxo-para-os-dados-de-cid","title":"Fluxo para os dados de CID","text":""},{"location":"#extract","title":"Extract","text":"<p>Para o arquivo, utilizamos o processador GetFile do Apache Nifi, com a seguinte configura\u00e7\u00e3o:</p> <p></p> <ul> <li>O Input Directory cont\u00e9m o caminho relativo a pasta <code>/opt/nifi/nifi-current/data-in</code> do container, observe que a   pasta <code>data-in</code>  est\u00e1 em um bind com a pasta local: data, por meio da configura\u00e7\u00e3o   no <code>docker-compose</code>:</li> </ul> <pre><code>volumes:\n- \"./data:/opt/nifi/nifi-current/data-in\"  </code></pre> <p>O filtro por nome foi inserido para que o processador n\u00e3o recupere outros arquivos da pasta. Durante o per\u00edodo de testes recomenda-se utilizar a propriedade \"Keep Source File\" como <code>true</code>, para que o arquivo original n\u00e3o seja deletado ap\u00f3s o processamento.</p> <p>O processador GetFile tem uma sa\u00edda poss\u00edvel, presente na aba \"Relationships\", e ficar\u00e1 no estado inv\u00e1lido at\u00e9 que as sa\u00eddas sejam tratadas.</p> <p></p> <p>Como a sa\u00edda desse processador ser\u00e1 o arquivo <code>CSV</code> lido, o pr\u00f3ximo processador ser\u00e1 respons\u00e1vel por fazer a leitura desse arquivo CSV e separ\u00e1-lo em arquivos contendo apenas um registro, j\u00e1 no formato <code>JSON</code>. Para isso, o processador * SplitRecord* ser\u00e1 utilizado, para ligar os dois processadores basta passar o mouse por cima do primeiro e quando a seta representada na figura abaixo aparecer, arrastar o mouse at\u00e9 o segundo processador.</p> <p></p> <p>A seguir, ser\u00e1 solicitada a configura\u00e7\u00e3o da conex\u00e3o, onde a sa\u00edda em caso de <code>sucess</code> do primeiro processador (GetFile) ser\u00e1 direcionada para o processador SplitRecord.</p> <p></p> <p>Nesse momento ser\u00e1 criada uma fila (queue) entre os processadores, onde ser\u00e1 poss\u00edvel acompanhar o estado do fluxo de dados.</p> <p></p> <p>Observe que o processador GetFile agora est\u00e1 no estado v\u00e1lido, mas ainda n\u00e3o foi iniciado, e o processador * SplitRecord* est\u00e1 no estado inv\u00e1lido.</p> <p>Para verificar o funcionamento do processador GetFile ele ser\u00e1 executado apenas uma vez, para isso clique com o bot\u00e3o direito do mouse sobre o processador e selecione a op\u00e7\u00e3o Run Once:</p> <p></p> <p>Ap\u00f3s a execu\u00e7\u00e3o, o arquivo lido (1 arquivo de 26.42 KB) entrar\u00e1 para a fila, aguardando a entrada no pr\u00f3ximo processador:</p> <p></p> <p>\u00c9 poss\u00edvel verificar os arquivos na fila selecionando a op\u00e7\u00e3o List Queue da fila:</p> <p></p> <p></p>"},{"location":"#transform","title":"Transform","text":"<p>O processador SplitRecord cont\u00e9m a primeira etapa da transforma\u00e7\u00e3o dos dados, a transforma\u00e7\u00e3o de <code>CSV</code> para <code>JSON</code>, para isso ele utilizar\u00e1 dois servi\u00e7os, um de leitura CSV e outro de escrita JSON. Esses servi\u00e7os devem ser configurados de forma global para o fluxo, para isso clique em um espa\u00e7o vazio do fluxo e selecione a op\u00e7\u00e3o Configure.</p> <p></p> <p>Na aba controller services selecione a op\u00e7\u00e3o para adicionar um novo servi\u00e7o, e busque por CSVReader.</p> <p></p> <p>Clique no \u00edcone de engrenagem para configurar o leitor e nas propriedades altere as seguintes configura\u00e7\u00f5es:</p> Configura\u00e7\u00e3o Valor Descri\u00e7\u00e3o Schema Access Strategy Use Strings Fields From Header Determina como os nomes das propriedades ser\u00e3o determinados. Value Separator ; Separador de registros, no caso do arquivo usado \u00e9 ';'. Treat First Line as Header true Auto explicativo. <p>Para ativar o servi\u00e7o, clique no \u00edcone de raio e no bot\u00e3o \"Enable\" da tela que aparecer.</p> <p>Repita os passos anteriores para adicionar os servi\u00e7os de leitura e escrita em JSON: JsonRecordSetWriter e JsonTreeReader. N\u00e3o \u00e9 necess\u00e1rio alterar nenhuma configura\u00e7\u00e3o.</p> <p>Agora que os servi\u00e7os de leitura e escrita est\u00e3o criados e habilitados, \u00e9 poss\u00edvel inseri-los na configura\u00e7\u00e3o do SplitRecord.</p> <p></p> <p>O processador SplitRecord tem 3 sa\u00eddas poss\u00edveis, e \u00e9 necess\u00e1rio direcion\u00e1-las ou configur\u00e1-las como finais, na aba relationships.</p> <p></p> <p>A sa\u00edda splits ser\u00e1 direcionada para o pr\u00f3ximo processador, para o tratamento dos dados por meio de um script python, o ExecuteScript.</p> <p>Configure o processador da seguinte maneira:</p> <p></p> <p>Observe que a pasta scripts est\u00e1 em um bind com a pasta <code>/opt/nifi/nifi-current/scripts</code> do container, por meio da configura\u00e7\u00e3o do docker-compose. O script <code>cid10_format_json.py</code> \u00e9 respons\u00e1vel pelo seguinte processamento:</p> format_json.python<pre><code>\n</code></pre> <p>Ap\u00f3s o processamento, cada arquivo encontra-se no seguinte estado, estando pronto para a inser\u00e7\u00e3o na base de dados.</p> <p></p> <p>O processador respons\u00e1vel pela convers\u00e3o de JSON para SQL \u00e9 o ConvertJSONtoSQL, como o pr\u00f3prio nome sugere, que necessita de uma JDBC Connection Pool para funcionar, portanto \u00e9 necess\u00e1rio cri\u00e1-la junto com os servi\u00e7os de leitura e escrito criados anteriormente.</p> <p></p> <p>A senha para o usu\u00e1rio apache tamb\u00e9m deve ser inserida, n\u00e3o aparecendo na imagem por ser um dado sens\u00edvel. O usu\u00e1rio apache tamb\u00e9m deve ser criado no banco de dados, caso n\u00e3o existe, e ter acesso aos objetos da base <code>sim_datasus</code>.</p> <p>Uma vez criada a pool de conex\u00f5es, basta configurar o processador da seguinte maneira: </p>"},{"location":"#load","title":"Load","text":"<p>Por fim, a inser\u00e7\u00e3o dos dados \u00e9 feita com aux\u00edlio do processador ExecuteSQL, que ir\u00e1 executar os arquivos sql gerados pelo processador anterior. Basta configurar a connection pool e os relationships como terminais.</p> <p></p> <p>Quantidade de tuplas antes da inser\u00e7\u00e3o:</p> <p></p> <p>Ap\u00f3s a inser\u00e7\u00e3o:</p> <p></p> <p></p>"},{"location":"#overview","title":"Overview","text":"<p>Fluxo completo:</p> <p></p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>[1] Arquivos em Formato CSV - CID-10, dispon\u00edvel em http://www2.datasus.gov.br/cid10/V2008/descrcsv.htm. Acesso em 12 de junho de 2023.</p> <p>[2] Sistema de Informa\u00e7\u00e3o sobre Mortalidade \u2013 SIM, dispon\u00edvel em  https://opendatasus.saude.gov.br/dataset/sim. Acesso em 12 de junho de 2023.</p> <p>[3] Exemplo de Script Jython, dispon\u00edvel em https://gist.github.com/ijokarumawak/1df6d34cd1b2861eb6b7432ee7245ccd</p> <p>[4] https://www.youtube.com/watch?v=cHElJ8M5g0Y&amp;ab_channel=InsightByte</p>"},{"location":"contexto/","title":"Contextualiza\u00e7\u00e3o","text":"<p>ETL \u00e9 a sigla para as opera\u00e7\u00f5es de extract, transform, load, que combinam dados de m\u00faltiplas fontes em um conjunto consistente armazenado em um armaz\u00e9m de dados ou outro sistema de destino. Esse processo \u00e9 essencial para projetos de armaz\u00e9m de dados_ e fornece a base para an\u00e1lise de dados e aprendizado de m\u00e1quina.</p> <p>O processo de ETL (Figura 1) consiste em tr\u00eas etapas, sendo a primeira a extra\u00e7\u00e3o, copiando ou exportando dados de diversas origens para uma \u00e1rea de staging, que pode incluir servidores SQL ou NoSQL, arquivos, email e p\u00e1ginas web. Em seguida, ocorre a transforma\u00e7\u00e3o, que envolve processar e consolidar os dados para o uso desejado. As tarefas podem incluir filtragem, limpeza, valida\u00e7\u00e3o, c\u00e1lculos, edi\u00e7\u00e3o de texto, convers\u00e3o de unidades de medida e formata\u00e7\u00e3o de dados em tabelas compat\u00edveis com o armaz\u00e9m de dados de destino. A \u00faltima etapa \u00e9 o carregamento dos dados no armaz\u00e9m de dados de destino, sendo geralmente automatizado e pode incluir uma carga inicial de todos os dados processados, seguida por carregamentos peri\u00f3dicos de altera\u00e7\u00f5es e, com menor frequ\u00eancia, recarregamento completo dos dados.</p> <p>A etapa de transforma\u00e7\u00e3o dos dados \u00e9 uma das mais cruciais em armaz\u00e9ns de dados, j\u00e1 que estes sistemas visam melhorar a qualidade dos dados armazenados. Os seguintes a seguir s\u00e3o frequentes nos dados obtidos na fase de extra\u00e7\u00e3o, e o processo de limpeza dos dados visa solucion\u00e1-los, geralmente por meio de ferramentas espec\u00edficas para ETL, que fornecem funcionalidades para a retifica\u00e7\u00e3o e homogeneiza\u00e7\u00e3o dos dados.</p> <ul> <li>Dados duplicados;</li> <li>Inconsist\u00eancia de dados;</li> <li>Dados ausentes;</li> <li>Uso indevido de campos - como, por exemplo, um telefone armazenado em um campo de documento de CPF;</li> <li>Valores imposs\u00edveis ou errados - como, por exemplo, uma data 30/02/1997;</li> <li>Valores inconsistentes entre entidades por conven\u00e7\u00f5es diferentes - como, por exemplo, uma entidade com o campo UF preenchido como \"DF\" e outra como \"Distrito Federal\";</li> <li>Valores inconsistentes por erros de digita\u00e7\u00e3o.</li> </ul> <p>As principais atividades da transforma\u00e7\u00e3o de dados para a convers\u00e3o dos dados em um formato espec\u00edfico para o armaz\u00e9m de dados, que objetiva estabelecer um mapeamento entre os dados de entrada e os dados do destino, s\u00e3o:</p> <ul> <li>Convers\u00e3o e normaliza\u00e7\u00e3o nas duas pontas dos dados, fonte e destino, para unificar formatos e unidades de medida;</li> <li>Mapeamento, para associar os campos equivalentes nas fontes e no destino;</li> <li>Sele\u00e7\u00e3o, que reduz o n\u00famero de campos nas fontes.</li> </ul> <p></p> <p>Figura 1: Processo de ETL. Fonte: [1]</p>"},{"location":"contexto/#objetivos-e-exemplos-de-uso","title":"Objetivos e Exemplos de Uso","text":"<p>O processo de ETL combina dados de diferentes fontes em um armazenamento de dados \u00fanico e consistente, que \u00e9 carregado em um armaz\u00e9m de dados ou outro sistema de destino, \u00e9 um dos principais m\u00e9todos de processamento de dados. Esse processo \u00e9 frequentemente utilizado por organiza\u00e7\u00f5es em processos de:</p> <ul> <li>Extra\u00e7\u00e3o de dados de sistemas legados: a jun\u00e7\u00e3o de bases de dados de sistemas legados pode se tornar um processo trabalhoso, pois estes sistemas muitas vezes utilizam vers\u00f5es anteriores dos sistemas gerenciadores de bancos de dados, armazenam dados em formatos diferentes e geralmente cont\u00e9m dados duplicados, como por exemplo os dados do Sistema de Informa\u00e7\u00e3o sobre Mortalidade, escolhido para a realiza\u00e7\u00e3o do trabalho, que cont\u00e9m diversas colunas para o registro da escolaridade da pessoa falecida e seus familiares, em detrimento da mudan\u00e7a de legisla\u00e7\u00e3o sobre o ensino b\u00e1sico, em 2010, que alterou a nomenclatura e quantidade de s\u00e9ries do ensino fundamental de 8 para 9 s\u00e9ries.</li> <li>Limpeza de dados para melhorar qualidade e prover consist\u00eancia: a duplica\u00e7\u00e3o de dados e armazenamento de dados derivados, dentre outras causas, podem gerar inconsist\u00eancia nos dados, o que dificulta a an\u00e1lise dos dados da maneira correta, podendo mascarar interfer\u00eancias entre dados.</li> <li>Carregamento de dados em bancos de dados: comumente \u00e9 necess\u00e1rio inicializar uma base de dados com valores j\u00e1 existentes, por\u00e9m n\u00e3o formatados, processo que pode ser facilitado e automatizado com ferramentas ETL.</li> </ul> <p>De forma geral, o processo de ETL \u00e9 essencial para o tratamento e corre\u00e7\u00e3o de dados provenientes de fontes diferentes e ou suscet\u00edveis a erros, um exemplo pr\u00e1tico \u00e9 o tratamento dos dados sobre clientes em lojas, que geralmente s\u00e3o inseridos no momento do pagamento, situa\u00e7\u00e3o em que existe uma probabilidade de erros de digita\u00e7\u00e3o, por exemplo.</p>"},{"location":"contexto/#ferramentas-e-aplicacoes","title":"Ferramentas e Aplica\u00e7\u00f5es","text":"<p>Inicialmente as organiza\u00e7\u00f5es escreviam seus pr\u00f3prios c\u00f3digos ETL para resolver problemas espec\u00edficos, mas atualmente est\u00e3o dispon\u00edveis ferramentas ETL de c\u00f3digo livre ou comerciais no mercado. Tais solu\u00e7\u00f5es, geralmente, incluem recursos como a automa\u00e7\u00e3o abrangente, suporte para gerenciamento de dados complexos, seguran\u00e7a e conformidade, al\u00e9m de apresentarem-se em interfaces de f\u00e1cil uso, de forma que o profissional de dados possa focar mais nas opera\u00e7\u00f5es realizadas e menos em c\u00f3digo e implementa\u00e7\u00e3o manual de scripts.</p> <p>Dentre as ferramentas dispon\u00edveis, algumas das de c\u00f3digo aberto mais utilizadas s\u00e3o:</p> <ol> <li>Apache NiFi: Uma ferramenta de integra\u00e7\u00e3o de dados de fluxo cont\u00ednuo, desenvolvida pela Apache Software Foundation. Ela permite extrair, transformar e carregar dados de forma f\u00e1cil e escal\u00e1vel, com uma interface visual intuitiva e recursos avan\u00e7ados.</li> <li>Pentaho Data Integration: Tamb\u00e9m conhecido como Kettle, \u00e9 uma solu\u00e7\u00e3o ETL de c\u00f3digo aberto da Pentaho. Ele fornece recursos completos de extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carregamento de dados, juntamente com recursos avan\u00e7ados de orquestra\u00e7\u00e3o e agendamento.</li> <li>CloverDX: Uma plataforma de integra\u00e7\u00e3o de dados escal\u00e1vel e flex\u00edvel, oferecendo recursos de ETL e transforma\u00e7\u00e3o de dados. Ela suporta uma ampla variedade de fontes de dados e possui uma interface gr\u00e1fica intuitiva para a cria\u00e7\u00e3o de fluxos de trabalho de ETL.</li> <li>Apache Airflow: Embora seja mais conhecido como uma ferramenta de orquestra\u00e7\u00e3o de fluxo de trabalho, o Apache Airflow tamb\u00e9m pode ser usado para tarefas de ETL. Ele permite definir e agendar fluxos de trabalho de ETL complexos, oferecendo extensibilidade e escalabilidade.</li> </ol>"},{"location":"contexto/#vantagens-e-desvantagens","title":"Vantagens e Desvantagens","text":"<p>O processo de ETL melhora a qualidade dos dados ao realizar a limpeza e tratamento antes de inserir em um outro reposit\u00f3rio, possui um grande repert\u00f3rio de ferramentas dispon\u00edveis, de f\u00e1cil uso e bem documentadas, mas pode tornar-se um processo demorado e trabalhoso a depender do volume de dados e quantidade de atualiza\u00e7\u00f5es subsequentes. Em situa\u00e7\u00f5es com reposit\u00f3rios maiores ou maior n\u00famero de atualiza\u00e7\u00f5es, recomenda-se a utiliza\u00e7\u00e3o de outras t\u00e9cnicas, como o ELT (Extract, Load, Transform) ou CDC (Captura de Dados de Mudan\u00e7a).</p>"},{"location":"contexto/#base-de-dados","title":"Base de Dados","text":""},{"location":"contexto/#fontes-de-dados","title":"Fontes de Dados","text":"<p>A base de dados que passar\u00e1 pelo processo de ETL durante a execu\u00e7\u00e3o do trabalho combina indicadores sobre a mortalidade no Brasil, no ano de 2022. A escolha foi motivada pela relev\u00e2ncia dos dados, uma vez que estes s\u00e3o os dados mais recentes, no portal de dados abertos do SUS, na data corrente. Nesta base est\u00e3o presentes, dentre outros, os dados relativos \u00e0 causa de morte, localidade e dados censit\u00e1rios do falecido. Tais dados s\u00e3o essenciais para a gera\u00e7\u00e3o de informa\u00e7\u00f5es como \u00edndice de mortalidade por faixa et\u00e1ria, acompanhamento do n\u00edvel de viol\u00eancia nas diversas regi\u00f5es do pa\u00eds e determina\u00e7\u00e3o das doen\u00e7as mais letais em brasileiro, sendo todas essas informa\u00e7\u00f5es relevantes durante a cria\u00e7\u00e3o de pol\u00edticas p\u00fablicas de seguran\u00e7a e sa\u00fade.</p> <p>A base de dados est\u00e1 dispon\u00edvel no s\u00edtio OpenDataSUS no formato CSV (Comma separated values) al\u00e9m de sua documenta\u00e7\u00e3o no formato de dicion\u00e1rio de dados. Ap\u00f3s uma an\u00e1lise da documenta\u00e7\u00e3o da base de dados, \u00e9 poss\u00edvel concluir que os dados disponibilizados s\u00e3o provenientes de fontes diferentes (Figura \\ref{origem-dd})</p> <p></p> <p>Figura 2: Documenta\u00e7\u00e3o da coluna \"ORIGEM\" da tabela. Fonte: [3]</p> <p>A tabela cujos dados est\u00e3o dispon\u00edveis possui 87 colunas, mas para a execu\u00e7\u00e3o do trabalho optou-se pela cria\u00e7\u00e3o de novas entidades e relacionamentos, visando diminuir a quantidade de redund\u00e2ncia e aumentar a integridade dos dados armazenados. Al\u00e9m disso, foi poss\u00edvel identificar colunas repetidas, em detrimento das diferentes fontes de dados e altera\u00e7\u00f5es nas legisla\u00e7\u00f5es brasileiras, como por exemplo as colunas indicando a escolaridade do falecido. Para atribuir maior legilibidade aos dados, ser\u00e3o utilizados os dados das CIDs (Classifica\u00e7\u00e3o Internacional de Doen\u00e7as) disponibilizados pelo DataSus.</p>"},{"location":"contexto/#documentacao-da-base-de-dados","title":"Documenta\u00e7\u00e3o da Base de Dados","text":"<p>O Diagrama Entidade Relacionamento visa representar graficamente as entidades e relacionamentos presentes na modelagem. A partir dos dados provenientes das duas fontes definidas, foram identificadas as entidades e relacionamentos representadas na Figura 3.</p> <p></p> <p>Figura 3: Diagrama Entidade Relacionamento (DER). Fonte: Autoria pr\u00f3pria.</p> <p>A partir das informa\u00e7\u00f5es contidas no Dicion\u00e1rio de Dados [3] e nos dados armazenados na base foi poss\u00edvel determinar os tipos de dados armazenados. Dando in\u00edcio \u00e0 etapa de limpeza dos dados, as colunas duplicadas foram removidas e os tipos de dados foram alterados para fornecer uma melhor adequa\u00e7\u00e3o e an\u00e1lise posterior.</p> <p> Figura 4: Diagrama L\u00f3gico de Dados (DLD). Fonte: Autoria pr\u00f3pria.</p>"},{"location":"contexto/#referencia-bibliografica","title":"Refer\u00eancia Bibliogr\u00e1fica","text":"<p>[1] GOLFARELLI M., RIZZI S. Data Warehouse Design: Modern Principles and Methodologies. 1.\u00aa edi\u00e7\u00e3o. Nova York, McGraw Hill, 2009.</p> <p>[2] What is ETL? \u2014 Artigo no site da IBM, dispon\u00edvel em https://www.ibm.com/topics/etl. Acesso em 12 de junho de 2023.</p> <p>[3] Sistema de Informa\u00e7\u00e3o sobre Mortalidade \u2013 SIM, dispon\u00edvel em https://opendatasus.saude.gov.br/dataset/sim. Acesso em 12 de junho de 2023.</p> <p>[4] Arquivos em Formato CSV - CID-10, dispon\u00edvel em http://www2.datasus.gov.br/cid10/V2008/descrcsv.htm. Acesso em 12 de junho de 2023.</p> <p>[5] Apache NiFi, Apache NiFi. Dispon\u00edvel em https://nifi.apache.org/. Acesso em 12 de junho de 2023.</p> <p>[6] Pentaho Data Integration, Pentaho Data Integration. Dispon\u00edvel em https://sourceforge.net/projects/pentaho/. Acesso em 12 de junho de 2023.</p> <p>[7] CloverDX, CloverDX. Dispon\u00edvel em https://www.cloverdx.com/. Acesso em 12 de junho de 2023.</p> <p>[8] Apache Airflow, Apache Airflow. Dispon\u00edvel em https://airflow.apache.org/. Acesso em 12 de junho de 2023.</p>"},{"location":"mortalidade/","title":"Dados sobre Mortalidade (SIM)","text":""},{"location":"mortalidade/#fluxo-para-os-dados-de-mortalidade-do-sim","title":"Fluxo para os dados de Mortalidade do SIM","text":""},{"location":"mortalidade/#extract","title":"Extract","text":"<p>O processador utilizado ser\u00e1 respons\u00e1vel por fazer a requisi\u00e7\u00e3o para o site que hospeda os arquivos CSV com os dados do SIM, sistema de informa\u00e7\u00f5es sobre mortalidade, do SUS.</p> <p>O nome deste processador \u00e9 InvokeHTTP, e nas suas configura\u00e7\u00f5es basta alterar a HTTP URL para o link da fonte de dados: https://diaad.s3.sa-east-1.amazonaws.com/sim/Mortalidade_Geral_2020.csv</p>"},{"location":"mortalidade/#transform","title":"Transform","text":"<p>Em seguida, o processador SplitRecords \u00e9 utilizado para fazer a convers\u00e3o dos dados de CSV para JSON. O mesmo processador utilizado no processo para o CID pode ser copiado e utilizado novamente.</p> <p>Diferentemente do processo do CID, no processo para os dados sobre mortalidade ser\u00e3o geradas tr\u00eas tabelas, conforme documentado no DER, portanto a sa\u00edda ser\u00e1 redirecionada para tr\u00eas processadores, com aux\u00edlio da ferramenta Funnel, conforme ilustra a figura:</p> <p></p> <p>Cada script ser\u00e1 respons\u00e1vel pela transforma\u00e7\u00e3o dos dados relativos a uma das tabelas.</p>"},{"location":"mortalidade/#metadados-sistema","title":"Metadados Sistema","text":"<p>Constam na tabela principal alguns dados de controle do sistema, portanto, foi criada uma nova entidade para armazenar esses dados.</p> fisico.sql <p>O script respons\u00e1vel por fazer esse tratamento \u00e9:</p> format_metadados.py"},{"location":"mortalidade/#investigacao","title":"Investiga\u00e7\u00e3o","text":"<p>O mesmo vale para os dados de investiga\u00e7\u00e3o.</p> fisico.sql <p>O script respons\u00e1vel por fazer esse tratamento \u00e9:</p> sim_format_investigacao.py"},{"location":"mortalidade/#pessoa-falecida","title":"Pessoa Falecida","text":"<p>O mesmo vale para os dados de pessoa falecida.</p> fisico.sql <p>O script respons\u00e1vel por fazer esse tratamento \u00e9:</p> sim_format_dados_pessoa_falecida.py"},{"location":"mortalidade/#obito","title":"\u00d3bito","text":"<p>O mesmo vale para os dados de \u00f3bito.</p> fisico.sql <p>O script respons\u00e1vel por fazer esse tratamento \u00e9:</p> sim_format_obito.py"},{"location":"setup/","title":"Configura\u00e7\u00e3o","text":""},{"location":"setup/#apache-nifi","title":"Apache Nifi","text":""},{"location":"setup/#docker-compose","title":"Docker Compose","text":"docker-compose.yaml<pre><code>version: \"3.3\"\nservices:\napache-nifi: build:\ncontext: .\ndockerfile: Dockerfile\ncontainer_name: apache-nifi\ndeploy:\nresources:\nlimits:\ncpus: '0.8'\nmemory: 4GB\nrestart: unless-stopped\nports:\n- \"8443:8443\"\nvolumes: - \"./data:/opt/nifi/nifi-current/data-in\"\n- \"./scripts:/opt/nifi/nifi-current/scripts\"\n- nifi_database:/opt/nifi/nifi-current/nifi_database\n- flow_storage:/opt/nifi/nifi-current/flow_storage\nnetworks:\n- mysql\nvolumes:\nnifi_database:\nname: nifi_database\nflow_storage:\nname: nifi_flow_storage\nnetworks:\nmysql:\nexternal: true\n</code></pre>"},{"location":"setup/#dockerfile","title":"Dockerfile","text":"Dockerfile<pre><code>FROM apache/nifi USER root \n# Instalando python no servidor.\nRUN apt-get update &amp;&amp; apt-get install python3 -y\n\nUSER nifi\nWORKDIR /opt/nifi/nifi-current/\n# Adicionando driver de conex\u00e3o com o mysql.\nRUN mkdir drivers\nCOPY apache/mysql-connector-j-8.0.33.jar drivers\n\n# Aumentando mem\u00f3ria da JVM para lidar com arquivos maiores.\nRUN sed -i 's/512m/2048m/g' conf/bootstrap.conf\n\n# Criando senha de acesso \u00fanico ao apache.\nRUN bin/nifi.sh set-single-user-credentials user apachenifi123\n\nCMD bin/nifi.sh run\n</code></pre>"},{"location":"setup/#rodando","title":"Rodando","text":"<p>Para rodar a imagem personalizada do apache nifi, com driver para conex\u00e3o jdbc com o MySQL e usu\u00e1rio:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"setup/#banco-de-dados","title":"Banco de Dados","text":""},{"location":"setup/#docker-compose_1","title":"Docker Compose","text":"docker-compose.yaml<pre><code>version: \"3.3\"\nservices:\nmysql:\nimage: mysql:8.0\ncontainer_name: mysql\nrestart: always\nports:\n- \"3306:3306\"\nenvironment:\n- MYSQL_ALLOW_EMPTY_PASSWORD=1\nvolumes:\n- mysql:/var/lib/mysql\nnetworks:\n- mysql\nvolumes:\nmysql:\nexternal: true\nnetworks:\nmysql:\nexternal: true\n</code></pre>"},{"location":"setup/#scripts-iniciais","title":"Scripts Iniciais","text":"fisico.sql controle.sql <p>Para limpar a base ap\u00f3s os testes, basta executar o script:</p> apaga.sql"}]}